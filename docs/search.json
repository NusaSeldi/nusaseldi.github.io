[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nusa Seldi",
    "section": "",
    "text": "Hello!\nI am Nusa Seldi Wibisono. Learning about data and enjoy spending time listening to music."
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post! Start of the journey!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nBooks Dashboard\n\n\n\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSpaceship Titanic\n\n\n\n\n\n\nJul 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAmes Housing Price Prediction\n\n\n\n\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Classification with XGboost and Resampling Method\n\n\n\n\n\n\nMay 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuild several models with workflowsets in tidymodels\n\n\n\n\n\n\nMay 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nApr 29, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/workflows-tidymodels/index.html",
    "href": "blog/posts/workflows-tidymodels/index.html",
    "title": "Build several models with workflows in tidymodels",
    "section": "",
    "text": "Motor Trend Car Road Tests (mtcars) contains the data from Motor Trend US magazine about fuel consumption and other aspects of automobile design and performance for 32 automobiles (1973-74 models).\nThis time, our goal is to predict the Miles per gallon (mpg) of 32 automobiles. We will utilize tidymodels’ workflow_set function to build several regression models at once: linear model, support vector machine, and xgboost."
  },
  {
    "objectID": "blog/posts/workflows-tidymodels/index.html#introduction",
    "href": "blog/posts/workflows-tidymodels/index.html#introduction",
    "title": "Build several models with workflows in tidymodels",
    "section": "",
    "text": "Motor Trend Car Road Tests (mtcars) contains the data from Motor Trend US magazine about fuel consumption and other aspects of automobile design and performance for 32 automobiles (1973-74 models).\nThis time, our goal is to predict the Miles per gallon (mpg) of 32 automobiles. We will utilize tidymodels’ workflow_set function to build several regression models at once: linear model, support vector machine, and xgboost."
  },
  {
    "objectID": "blog/posts/workflows-tidymodels/index.html#import-library",
    "href": "blog/posts/workflows-tidymodels/index.html#import-library",
    "title": "Build several models with workflows in tidymodels",
    "section": "Import Library",
    "text": "Import Library\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(skimr)"
  },
  {
    "objectID": "blog/posts/workflows-tidymodels/index.html#load-the-data",
    "href": "blog/posts/workflows-tidymodels/index.html#load-the-data",
    "title": "Build several models with workflows in tidymodels",
    "section": "Load the data",
    "text": "Load the data\n\ncar_df &lt;- mtcars\n\nglimpse(car_df)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\nskim(car_df)\n\n\nData summary\n\n\nName\ncar_df\n\n\nNumber of rows\n32\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmpg\n0\n1\n20.09\n6.03\n10.40\n15.43\n19.20\n22.80\n33.90\n▃▇▅▁▂\n\n\ncyl\n0\n1\n6.19\n1.79\n4.00\n4.00\n6.00\n8.00\n8.00\n▆▁▃▁▇\n\n\ndisp\n0\n1\n230.72\n123.94\n71.10\n120.83\n196.30\n326.00\n472.00\n▇▃▃▃▂\n\n\nhp\n0\n1\n146.69\n68.56\n52.00\n96.50\n123.00\n180.00\n335.00\n▇▇▆▃▁\n\n\ndrat\n0\n1\n3.60\n0.53\n2.76\n3.08\n3.70\n3.92\n4.93\n▇▃▇▅▁\n\n\nwt\n0\n1\n3.22\n0.98\n1.51\n2.58\n3.33\n3.61\n5.42\n▃▃▇▁▂\n\n\nqsec\n0\n1\n17.85\n1.79\n14.50\n16.89\n17.71\n18.90\n22.90\n▃▇▇▂▁\n\n\nvs\n0\n1\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nam\n0\n1\n0.41\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\ngear\n0\n1\n3.69\n0.74\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▆▁▂\n\n\ncarb\n0\n1\n2.81\n1.62\n1.00\n2.00\n2.00\n4.00\n8.00\n▇▂▅▁▁\n\n\n\n\n\nIt can be seen that the dataset consists of 32 automobiles with 11 variables. All data types are numeric and there are no missing values in the dataset."
  },
  {
    "objectID": "blog/posts/workflows-tidymodels/index.html#preprocessing",
    "href": "blog/posts/workflows-tidymodels/index.html#preprocessing",
    "title": "Build several models with workflows in tidymodels",
    "section": "Preprocessing",
    "text": "Preprocessing\nFor preprocessing, we will change two variables, am (transmission) and vs (engine). We will transform the format to factor and also change the labels of the values.\n\ncar_df &lt;- car_df |&gt; \n  mutate( am = case_match(am, 1 ~ \"manual\", .default = 'automatic'),\n          vs = case_match(vs, 1 ~ 'straight', .default = 'v-shaped'),\n          am = as.factor(am),\n          vs = as.factor(vs))"
  },
  {
    "objectID": "blog/posts/workflows-tidymodels/index.html#explore-the-data",
    "href": "blog/posts/workflows-tidymodels/index.html#explore-the-data",
    "title": "Build several models with workflows in tidymodels",
    "section": "Explore the data",
    "text": "Explore the data"
  },
  {
    "objectID": "blog/posts/workflows-tidymodels/index.html#build-a-model",
    "href": "blog/posts/workflows-tidymodels/index.html#build-a-model",
    "title": "Build several models with workflows in tidymodels",
    "section": "Build a Model",
    "text": "Build a Model\nBefore we build a model, we will divide the data into training set and test set with a ratio of 80:20. For feature engineering, we will normalize the data for numeric data and create dummy variables for nominal data.\n\nset.seed(11)\ncar_split &lt;- initial_split(car_df, prop = 0.8)\ncar_train &lt;- training(car_split)\ncar_test &lt;- testing(car_split)\n\nset.seed(80)\ncar_fold &lt;- bootstraps(car_train, times = 10)\n\ncar_recipe &lt;- recipe(mpg ~ ., data = car_train) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_dummy(all_nominal_predictors())\n\nlm_spec &lt;- linear_reg() |&gt; \n  set_mode('regression') |&gt; \n  set_engine('stan')\n\nxgb_spec &lt;- boost_tree() |&gt; \n  set_mode('regression') |&gt; \n  set_engine('xgboost')\n\nsvm_spec &lt;- svm_linear() |&gt; \n  set_mode('regression') |&gt; \n  set_engine('kernlab')\n\nwf_set &lt;- workflow_set(preproc = list(basic = car_recipe),\n                       models = list(lm = lm_spec,\n                                     xgboost = xgb_spec,\n                                     svm = svm_spec))\n\nwf_set_fit &lt;- workflow_map(wf_set,\n                           resamples = car_fold,\n                           seed = 123,\n                           control = control_grid(save_pred = TRUE, save_workflow = TRUE ,parallel_over = \"everything\"))"
  },
  {
    "objectID": "blog/posts/workflows-tidymodels/index.html#evaluate-the-model",
    "href": "blog/posts/workflows-tidymodels/index.html#evaluate-the-model",
    "title": "Build several models with workflows in tidymodels",
    "section": "Evaluate the model",
    "text": "Evaluate the model\nFor the evaluation, we will use metric rmse to estimate our model performance. From the three models, we choose the best model according to metric rmse and fit the final model to the training set and evaluate the test set.\n\nwf_set_fit |&gt; collect_metrics(summarize = T) \n\n# A tibble: 6 × 9\n  wflow_id      .config     preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 basic_lm      Preprocess… recipe  line… rmse    standard   5.57     10  0.245 \n2 basic_lm      Preprocess… recipe  line… rsq     standard   0.433    10  0.0540\n3 basic_xgboost Preprocess… recipe  boos… rmse    standard   3.57     10  0.193 \n4 basic_xgboost Preprocess… recipe  boos… rsq     standard   0.728    10  0.0265\n5 basic_svm     Preprocess… recipe  svm_… rmse    standard   4.46     10  0.272 \n6 basic_svm     Preprocess… recipe  svm_… rsq     standard   0.584    10  0.0486\n\nwf_set_fit |&gt; \n  rank_results() |&gt; \n  filter(.metric == 'rmse')\n\n# A tibble: 3 × 9\n  wflow_id      .config     .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 basic_xgboost Preprocess… rmse     3.57   0.193    10 recipe       boos…     1\n2 basic_svm     Preprocess… rmse     4.46   0.272    10 recipe       svm_…     2\n3 basic_lm      Preprocess… rmse     5.57   0.245    10 recipe       line…     3\n\nautoplot(wf_set_fit, rank_metric = 'rmse', metric = 'rmse', select_best = TRUE)\n\n\n\n\n\n\n\nbest_result &lt;- wf_set_fit |&gt; \n  extract_workflow_set_result(id = 'basic_xgboost') |&gt; \n  select_best(metric = 'rmse')\n\nxgboost_result &lt;- wf_set_fit |&gt; \n  extract_workflow('basic_xgboost') |&gt; \n  finalize_workflow(best_result) |&gt; \n  last_fit(split = car_split)\n\ncollect_metrics(xgboost_result) \n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       1.69  Preprocessor1_Model1\n2 rsq     standard       0.888 Preprocessor1_Model1\n\npredicted &lt;- xgboost_result |&gt; \n  collect_predictions()\n\npredicted |&gt; \n  select(.pred, mpg)\n\n# A tibble: 7 × 2\n  .pred   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1  21.7  22.8\n2  17.6  19.2\n3  17.1  15.2\n4  10.4  10.4\n5  23.2  26  \n6  18.4  19.7\n7  23.0  21.4\n\npredicted |&gt; \n  ggplot(aes(x = mpg, y = .pred)) +\n  geom_point() +\n  geom_abline(lty = 2) +\n  coord_obs_pred()"
  },
  {
    "objectID": "blog/posts/palmer-penguins/index.html",
    "href": "blog/posts/palmer-penguins/index.html",
    "title": "Palmer Penguins Classification with XGboost and Resampling Method",
    "section": "",
    "text": "This time, we will build a XGboost model to classify the gender of palmer penguins dataset. We also gonna use resampling method to measure how well our model performance."
  },
  {
    "objectID": "blog/posts/palmer-penguins/index.html#load-library",
    "href": "blog/posts/palmer-penguins/index.html#load-library",
    "title": "Palmer Penguins Classification with XGboost and Resampling Method",
    "section": "Load library",
    "text": "Load library\n\nlibrary(tidyverse) \nlibrary(tidymodels) \nlibrary(palmerpenguins) \nlibrary(vip)"
  },
  {
    "objectID": "blog/posts/palmer-penguins/index.html#dataset",
    "href": "blog/posts/palmer-penguins/index.html#dataset",
    "title": "Palmer Penguins Classification with XGboost and Resampling Method",
    "section": "Dataset",
    "text": "Dataset\n\npenguin_df &lt;- penguins\nglimpse(penguin_df)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "blog/posts/palmer-penguins/index.html#viz-the-dataset",
    "href": "blog/posts/palmer-penguins/index.html#viz-the-dataset",
    "title": "Palmer Penguins Classification with XGboost and Resampling Method",
    "section": "Viz the dataset",
    "text": "Viz the dataset\nFrom the visualization, we can say that male penguin bigger than female penguin in terms of body mass and flipper lenght.\n\npenguin_df &lt;- penguin_df |&gt;\n  drop_na(sex) |&gt;\n  select(-year, -island)\n\npenguin_df |&gt; ggplot(aes(bill_length_mm, bill_depth_mm, color = sex)) +\n  geom_point() +\n  facet_wrap(~species)\n\n\n\n\n\n\n\npenguin_df |&gt; ggplot(aes(species, body_mass_g, color = sex)) +\n  geom_boxplot()\n\n\n\n\n\n\n\npenguin_df |&gt; ggplot(aes(flipper_length_mm, body_mass_g, color = sex)) +\n  geom_point() +\n  facet_wrap(~species)"
  },
  {
    "objectID": "blog/posts/palmer-penguins/index.html#build-a-model",
    "href": "blog/posts/palmer-penguins/index.html#build-a-model",
    "title": "Palmer Penguins Classification with XGboost and Resampling Method",
    "section": "Build a model",
    "text": "Build a model\nBefore we build model, we split the data into training set and testing set. After that, we use resampling method called V-fold cross validation (CV) and build a xgboost model. For preprocessing, we impute the missing data with median and then normalize the numeric predictors and create dummy variable for categorical predictors.\n\nset.seed(99)\npenguin_split &lt;- initial_split(penguin_df, prop = 0.7, strata = sex)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\n\npenguin_fold &lt;- vfold_cv(data = penguin_train, strata = sex)\n\nbt_spec &lt;- boost_tree() |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"xgboost\")\n\npenguin_recipe &lt;- recipe(sex ~ ., data = penguin_train) |&gt;\n  step_impute_median(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors())\n\npenguin_wf &lt;- workflow() |&gt;\n  add_recipe(penguin_recipe) |&gt;\n  add_model(bt_spec)\n\nbt_fit &lt;- penguin_wf |&gt; fit_resamples(resamples = penguin_fold, control = control_resamples(save_pred = TRUE))"
  },
  {
    "objectID": "blog/posts/palmer-penguins/index.html#evaluating-the-model",
    "href": "blog/posts/palmer-penguins/index.html#evaluating-the-model",
    "title": "Palmer Penguins Classification with XGboost and Resampling Method",
    "section": "Evaluating the model",
    "text": "Evaluating the model\nAs we can see, there are 10 results created from the resampling. Last, we fit the test data and evaluate the model with accuracy and ROC, also create confusion matrix\n\ncollect_metrics(bt_fit, summarize = FALSE)\n\n# A tibble: 20 × 5\n   id     .metric  .estimator .estimate .config             \n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n 1 Fold01 accuracy binary         0.917 Preprocessor1_Model1\n 2 Fold01 roc_auc  binary         0.986 Preprocessor1_Model1\n 3 Fold02 accuracy binary         0.875 Preprocessor1_Model1\n 4 Fold02 roc_auc  binary         0.892 Preprocessor1_Model1\n 5 Fold03 accuracy binary         0.958 Preprocessor1_Model1\n 6 Fold03 roc_auc  binary         0.972 Preprocessor1_Model1\n 7 Fold04 accuracy binary         0.917 Preprocessor1_Model1\n 8 Fold04 roc_auc  binary         0.993 Preprocessor1_Model1\n 9 Fold05 accuracy binary         0.875 Preprocessor1_Model1\n10 Fold05 roc_auc  binary         0.917 Preprocessor1_Model1\n11 Fold06 accuracy binary         0.913 Preprocessor1_Model1\n12 Fold06 roc_auc  binary         0.970 Preprocessor1_Model1\n13 Fold07 accuracy binary         0.913 Preprocessor1_Model1\n14 Fold07 roc_auc  binary         0.962 Preprocessor1_Model1\n15 Fold08 accuracy binary         0.909 Preprocessor1_Model1\n16 Fold08 roc_auc  binary         0.959 Preprocessor1_Model1\n17 Fold09 accuracy binary         0.909 Preprocessor1_Model1\n18 Fold09 roc_auc  binary         0.983 Preprocessor1_Model1\n19 Fold10 accuracy binary         0.955 Preprocessor1_Model1\n20 Fold10 roc_auc  binary         1     Preprocessor1_Model1\n\npenguin_final &lt;- penguin_wf |&gt;\n  last_fit(penguin_split)\n\ncollect_metrics(penguin_final)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.901 Preprocessor1_Model1\n2 roc_auc  binary         0.960 Preprocessor1_Model1\n\nresult &lt;- collect_predictions(penguin_final)\n\nresult |&gt; conf_mat(sex, .pred_class)\n\n          Truth\nPrediction female male\n    female     41    1\n    male        9   50\n\npenguin_final |&gt;\n  extract_fit_parsnip() |&gt;\n  vip(aesthetics = list(fill = \"navy\"))"
  },
  {
    "objectID": "blog/posts/ames-housing/index.html",
    "href": "blog/posts/ames-housing/index.html",
    "title": "Ames Housing Price Prediction",
    "section": "",
    "text": "Predicting price of the house using multilayer perceptron model and also trying dimensionality reduction"
  },
  {
    "objectID": "blog/posts/ames-housing/index.html#introduction",
    "href": "blog/posts/ames-housing/index.html#introduction",
    "title": "Ames Housing Price Prediction",
    "section": "Introduction",
    "text": "Introduction\nHello! Today we will try to participate in another kaggle Getting Started competitions. The challenge in this competition is to predict the final price of each house using the data provided. For this competition, we will build a multilayer perceptron model to complete the challenge. We also gonna tune the parameters to optimize our model."
  },
  {
    "objectID": "blog/posts/ames-housing/index.html#import-library",
    "href": "blog/posts/ames-housing/index.html#import-library",
    "title": "Ames Housing Price Prediction",
    "section": "Import Library",
    "text": "Import Library\n\npacman::p_load(\ntidyverse,\ntidymodels,\nskimr,\nfastICA\n)"
  },
  {
    "objectID": "blog/posts/ames-housing/index.html#load-and-inspect-the-data",
    "href": "blog/posts/ames-housing/index.html#load-and-inspect-the-data",
    "title": "Ames Housing Price Prediction",
    "section": "Load and inspect the data",
    "text": "Load and inspect the data\nThere are 1460 observation and 81 variables for training dataset also 1459 observation and 80 variables for test dataset that we got from kaggle. For training dataset consist of 43 categorical data and 38 numeric data.\n\n  ames_train &lt;- read_csv(\"train.csv\")\n  ames_test &lt;- read_csv(\"test.csv\")\n  \n  dim(ames_train)\n\n[1] 1460   81\n\n  glimpse(ames_train)\n\nRows: 1,460\nColumns: 81\n$ Id            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ MSSubClass    &lt;dbl&gt; 60, 20, 60, 70, 60, 50, 20, 60, 50, 190, 20, 60, 20, 20,…\n$ MSZoning      &lt;chr&gt; \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RL\", \"RM\", \"R…\n$ LotFrontage   &lt;dbl&gt; 65, 80, 68, 60, 84, 85, 75, NA, 51, 50, 70, 85, NA, 91, …\n$ LotArea       &lt;dbl&gt; 8450, 9600, 11250, 9550, 14260, 14115, 10084, 10382, 612…\n$ Street        &lt;chr&gt; \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", \"Pave\", …\n$ Alley         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LotShape      &lt;chr&gt; \"Reg\", \"Reg\", \"IR1\", \"IR1\", \"IR1\", \"IR1\", \"Reg\", \"IR1\", …\n$ LandContour   &lt;chr&gt; \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", \"Lvl\", …\n$ Utilities     &lt;chr&gt; \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPub\", \"AllPu…\n$ LotConfig     &lt;chr&gt; \"Inside\", \"FR2\", \"Inside\", \"Corner\", \"FR2\", \"Inside\", \"I…\n$ LandSlope     &lt;chr&gt; \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", \"Gtl\", …\n$ Neighborhood  &lt;chr&gt; \"CollgCr\", \"Veenker\", \"CollgCr\", \"Crawfor\", \"NoRidge\", \"…\n$ Condition1    &lt;chr&gt; \"Norm\", \"Feedr\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\",…\n$ Condition2    &lt;chr&gt; \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", \"Norm\", …\n$ BldgType      &lt;chr&gt; \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", \"1Fam\", …\n$ HouseStyle    &lt;chr&gt; \"2Story\", \"1Story\", \"2Story\", \"2Story\", \"2Story\", \"1.5Fi…\n$ OverallQual   &lt;dbl&gt; 7, 6, 7, 7, 8, 5, 8, 7, 7, 5, 5, 9, 5, 7, 6, 7, 6, 4, 5,…\n$ OverallCond   &lt;dbl&gt; 5, 8, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 5, 5, 8, 7, 5, 5,…\n$ YearBuilt     &lt;dbl&gt; 2003, 1976, 2001, 1915, 2000, 1993, 2004, 1973, 1931, 19…\n$ YearRemodAdd  &lt;dbl&gt; 2003, 1976, 2002, 1970, 2000, 1995, 2005, 1973, 1950, 19…\n$ RoofStyle     &lt;chr&gt; \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"Gable\", \"G…\n$ RoofMatl      &lt;chr&gt; \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"CompShg\", \"…\n$ Exterior1st   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Sdng\", \"VinylSd\", \"…\n$ Exterior2nd   &lt;chr&gt; \"VinylSd\", \"MetalSd\", \"VinylSd\", \"Wd Shng\", \"VinylSd\", \"…\n$ MasVnrType    &lt;chr&gt; \"BrkFace\", \"None\", \"BrkFace\", \"None\", \"BrkFace\", \"None\",…\n$ MasVnrArea    &lt;dbl&gt; 196, 0, 162, 0, 350, 0, 186, 240, 0, 0, 0, 286, 0, 306, …\n$ ExterQual     &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ ExterCond     &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ Foundation    &lt;chr&gt; \"PConc\", \"CBlock\", \"PConc\", \"BrkTil\", \"PConc\", \"Wood\", \"…\n$ BsmtQual      &lt;chr&gt; \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Ex\", \"Gd\", \"TA\", \"T…\n$ BsmtCond      &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"Gd\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ BsmtExposure  &lt;chr&gt; \"No\", \"Gd\", \"Mn\", \"No\", \"Av\", \"No\", \"Av\", \"Mn\", \"No\", \"N…\n$ BsmtFinType1  &lt;chr&gt; \"GLQ\", \"ALQ\", \"GLQ\", \"ALQ\", \"GLQ\", \"GLQ\", \"GLQ\", \"ALQ\", …\n$ BsmtFinSF1    &lt;dbl&gt; 706, 978, 486, 216, 655, 732, 1369, 859, 0, 851, 906, 99…\n$ BsmtFinType2  &lt;chr&gt; \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"Unf\", \"BLQ\", …\n$ BsmtFinSF2    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ BsmtUnfSF     &lt;dbl&gt; 150, 284, 434, 540, 490, 64, 317, 216, 952, 140, 134, 17…\n$ TotalBsmtSF   &lt;dbl&gt; 856, 1262, 920, 756, 1145, 796, 1686, 1107, 952, 991, 10…\n$ Heating       &lt;chr&gt; \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", \"GasA\", …\n$ HeatingQC     &lt;chr&gt; \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"Ex\", \"Ex\", \"Ex\", \"Ex\", \"Gd\", \"E…\n$ CentralAir    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ Electrical    &lt;chr&gt; \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"SBrkr\", \"S…\n$ `1stFlrSF`    &lt;dbl&gt; 856, 1262, 920, 961, 1145, 796, 1694, 1107, 1022, 1077, …\n$ `2ndFlrSF`    &lt;dbl&gt; 854, 0, 866, 756, 1053, 566, 0, 983, 752, 0, 0, 1142, 0,…\n$ LowQualFinSF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ GrLivArea     &lt;dbl&gt; 1710, 1262, 1786, 1717, 2198, 1362, 1694, 2090, 1774, 10…\n$ BsmtFullBath  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,…\n$ BsmtHalfBath  &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FullBath      &lt;dbl&gt; 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1,…\n$ HalfBath      &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,…\n$ BedroomAbvGr  &lt;dbl&gt; 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 3, 4, 2, 3, 2, 2, 2, 2, 3,…\n$ KitchenAbvGr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ KitchenQual   &lt;chr&gt; \"Gd\", \"TA\", \"Gd\", \"Gd\", \"Gd\", \"TA\", \"Gd\", \"TA\", \"TA\", \"T…\n$ TotRmsAbvGrd  &lt;dbl&gt; 8, 6, 6, 7, 9, 5, 7, 7, 8, 5, 5, 11, 4, 7, 5, 5, 5, 6, 6…\n$ Functional    &lt;chr&gt; \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", \"Typ\", …\n$ Fireplaces    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 0, 1, 0, 0,…\n$ FireplaceQu   &lt;chr&gt; NA, \"TA\", \"TA\", \"Gd\", \"TA\", NA, \"Gd\", \"TA\", \"TA\", \"TA\", …\n$ GarageType    &lt;chr&gt; \"Attchd\", \"Attchd\", \"Attchd\", \"Detchd\", \"Attchd\", \"Attch…\n$ GarageYrBlt   &lt;dbl&gt; 2003, 1976, 2001, 1998, 2000, 1993, 2004, 1973, 1931, 19…\n$ GarageFinish  &lt;chr&gt; \"RFn\", \"RFn\", \"RFn\", \"Unf\", \"RFn\", \"Unf\", \"RFn\", \"RFn\", …\n$ GarageCars    &lt;dbl&gt; 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 3, 1, 2, 2, 2, 2,…\n$ GarageArea    &lt;dbl&gt; 548, 460, 608, 642, 836, 480, 636, 484, 468, 205, 384, 7…\n$ GarageQual    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"Fa\", \"G…\n$ GarageCond    &lt;chr&gt; \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"TA\", \"T…\n$ PavedDrive    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ WoodDeckSF    &lt;dbl&gt; 0, 298, 0, 0, 192, 40, 255, 235, 90, 0, 0, 147, 140, 160…\n$ OpenPorchSF   &lt;dbl&gt; 61, 0, 42, 35, 84, 30, 57, 204, 0, 4, 0, 21, 0, 33, 213,…\n$ EnclosedPorch &lt;dbl&gt; 0, 0, 0, 272, 0, 0, 0, 228, 205, 0, 0, 0, 0, 0, 176, 0, …\n$ `3SsnPorch`   &lt;dbl&gt; 0, 0, 0, 0, 0, 320, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ScreenPorch   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 176, 0, 0, 0, 0, 0, …\n$ PoolArea      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PoolQC        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Fence         &lt;chr&gt; NA, NA, NA, NA, NA, \"MnPrv\", NA, NA, NA, NA, NA, NA, NA,…\n$ MiscFeature   &lt;chr&gt; NA, NA, NA, NA, NA, \"Shed\", NA, \"Shed\", NA, NA, NA, NA, …\n$ MiscVal       &lt;dbl&gt; 0, 0, 0, 0, 0, 700, 0, 350, 0, 0, 0, 0, 0, 0, 0, 0, 700,…\n$ MoSold        &lt;dbl&gt; 2, 5, 9, 2, 12, 10, 8, 11, 4, 1, 2, 7, 9, 8, 5, 7, 3, 10…\n$ YrSold        &lt;dbl&gt; 2008, 2007, 2008, 2006, 2008, 2009, 2007, 2009, 2008, 20…\n$ SaleType      &lt;chr&gt; \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"WD\", \"W…\n$ SaleCondition &lt;chr&gt; \"Normal\", \"Normal\", \"Normal\", \"Abnorml\", \"Normal\", \"Norm…\n$ SalePrice     &lt;dbl&gt; 208500, 181500, 223500, 140000, 250000, 143000, 307000, …\n\n  skim(ames_train)\n\n\nData summary\n\n\nName\names_train\n\n\nNumber of rows\n1460\n\n\nNumber of columns\n81\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n43\n\n\nnumeric\n38\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nMSZoning\n0\n1.00\n2\n7\n0\n5\n0\n\n\nStreet\n0\n1.00\n4\n4\n0\n2\n0\n\n\nAlley\n1369\n0.06\n4\n4\n0\n2\n0\n\n\nLotShape\n0\n1.00\n3\n3\n0\n4\n0\n\n\nLandContour\n0\n1.00\n3\n3\n0\n4\n0\n\n\nUtilities\n0\n1.00\n6\n6\n0\n2\n0\n\n\nLotConfig\n0\n1.00\n3\n7\n0\n5\n0\n\n\nLandSlope\n0\n1.00\n3\n3\n0\n3\n0\n\n\nNeighborhood\n0\n1.00\n5\n7\n0\n25\n0\n\n\nCondition1\n0\n1.00\n4\n6\n0\n9\n0\n\n\nCondition2\n0\n1.00\n4\n6\n0\n8\n0\n\n\nBldgType\n0\n1.00\n4\n6\n0\n5\n0\n\n\nHouseStyle\n0\n1.00\n4\n6\n0\n8\n0\n\n\nRoofStyle\n0\n1.00\n3\n7\n0\n6\n0\n\n\nRoofMatl\n0\n1.00\n4\n7\n0\n8\n0\n\n\nExterior1st\n0\n1.00\n5\n7\n0\n15\n0\n\n\nExterior2nd\n0\n1.00\n5\n7\n0\n16\n0\n\n\nMasVnrType\n8\n0.99\n4\n7\n0\n4\n0\n\n\nExterQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nExterCond\n0\n1.00\n2\n2\n0\n5\n0\n\n\nFoundation\n0\n1.00\n4\n6\n0\n6\n0\n\n\nBsmtQual\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtCond\n37\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtExposure\n38\n0.97\n2\n2\n0\n4\n0\n\n\nBsmtFinType1\n37\n0.97\n3\n3\n0\n6\n0\n\n\nBsmtFinType2\n38\n0.97\n3\n3\n0\n6\n0\n\n\nHeating\n0\n1.00\n4\n5\n0\n6\n0\n\n\nHeatingQC\n0\n1.00\n2\n2\n0\n5\n0\n\n\nCentralAir\n0\n1.00\n1\n1\n0\n2\n0\n\n\nElectrical\n1\n1.00\n3\n5\n0\n5\n0\n\n\nKitchenQual\n0\n1.00\n2\n2\n0\n4\n0\n\n\nFunctional\n0\n1.00\n3\n4\n0\n7\n0\n\n\nFireplaceQu\n690\n0.53\n2\n2\n0\n5\n0\n\n\nGarageType\n81\n0.94\n6\n7\n0\n6\n0\n\n\nGarageFinish\n81\n0.94\n3\n3\n0\n3\n0\n\n\nGarageQual\n81\n0.94\n2\n2\n0\n5\n0\n\n\nGarageCond\n81\n0.94\n2\n2\n0\n5\n0\n\n\nPavedDrive\n0\n1.00\n1\n1\n0\n3\n0\n\n\nPoolQC\n1453\n0.00\n2\n2\n0\n3\n0\n\n\nFence\n1179\n0.19\n4\n5\n0\n4\n0\n\n\nMiscFeature\n1406\n0.04\n4\n4\n0\n4\n0\n\n\nSaleType\n0\n1.00\n2\n5\n0\n9\n0\n\n\nSaleCondition\n0\n1.00\n6\n7\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nId\n0\n1.00\n730.50\n421.61\n1\n365.75\n730.5\n1095.25\n1460\n▇▇▇▇▇\n\n\nMSSubClass\n0\n1.00\n56.90\n42.30\n20\n20.00\n50.0\n70.00\n190\n▇▅▂▁▁\n\n\nLotFrontage\n259\n0.82\n70.05\n24.28\n21\n59.00\n69.0\n80.00\n313\n▇▃▁▁▁\n\n\nLotArea\n0\n1.00\n10516.83\n9981.26\n1300\n7553.50\n9478.5\n11601.50\n215245\n▇▁▁▁▁\n\n\nOverallQual\n0\n1.00\n6.10\n1.38\n1\n5.00\n6.0\n7.00\n10\n▁▂▇▅▁\n\n\nOverallCond\n0\n1.00\n5.58\n1.11\n1\n5.00\n5.0\n6.00\n9\n▁▁▇▅▁\n\n\nYearBuilt\n0\n1.00\n1971.27\n30.20\n1872\n1954.00\n1973.0\n2000.00\n2010\n▁▂▃▆▇\n\n\nYearRemodAdd\n0\n1.00\n1984.87\n20.65\n1950\n1967.00\n1994.0\n2004.00\n2010\n▅▂▂▃▇\n\n\nMasVnrArea\n8\n0.99\n103.69\n181.07\n0\n0.00\n0.0\n166.00\n1600\n▇▁▁▁▁\n\n\nBsmtFinSF1\n0\n1.00\n443.64\n456.10\n0\n0.00\n383.5\n712.25\n5644\n▇▁▁▁▁\n\n\nBsmtFinSF2\n0\n1.00\n46.55\n161.32\n0\n0.00\n0.0\n0.00\n1474\n▇▁▁▁▁\n\n\nBsmtUnfSF\n0\n1.00\n567.24\n441.87\n0\n223.00\n477.5\n808.00\n2336\n▇▅▂▁▁\n\n\nTotalBsmtSF\n0\n1.00\n1057.43\n438.71\n0\n795.75\n991.5\n1298.25\n6110\n▇▃▁▁▁\n\n\n1stFlrSF\n0\n1.00\n1162.63\n386.59\n334\n882.00\n1087.0\n1391.25\n4692\n▇▅▁▁▁\n\n\n2ndFlrSF\n0\n1.00\n346.99\n436.53\n0\n0.00\n0.0\n728.00\n2065\n▇▃▂▁▁\n\n\nLowQualFinSF\n0\n1.00\n5.84\n48.62\n0\n0.00\n0.0\n0.00\n572\n▇▁▁▁▁\n\n\nGrLivArea\n0\n1.00\n1515.46\n525.48\n334\n1129.50\n1464.0\n1776.75\n5642\n▇▇▁▁▁\n\n\nBsmtFullBath\n0\n1.00\n0.43\n0.52\n0\n0.00\n0.0\n1.00\n3\n▇▆▁▁▁\n\n\nBsmtHalfBath\n0\n1.00\n0.06\n0.24\n0\n0.00\n0.0\n0.00\n2\n▇▁▁▁▁\n\n\nFullBath\n0\n1.00\n1.57\n0.55\n0\n1.00\n2.0\n2.00\n3\n▁▇▁▇▁\n\n\nHalfBath\n0\n1.00\n0.38\n0.50\n0\n0.00\n0.0\n1.00\n2\n▇▁▅▁▁\n\n\nBedroomAbvGr\n0\n1.00\n2.87\n0.82\n0\n2.00\n3.0\n3.00\n8\n▁▇▂▁▁\n\n\nKitchenAbvGr\n0\n1.00\n1.05\n0.22\n0\n1.00\n1.0\n1.00\n3\n▁▇▁▁▁\n\n\nTotRmsAbvGrd\n0\n1.00\n6.52\n1.63\n2\n5.00\n6.0\n7.00\n14\n▂▇▇▁▁\n\n\nFireplaces\n0\n1.00\n0.61\n0.64\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▁▁\n\n\nGarageYrBlt\n81\n0.94\n1978.51\n24.69\n1900\n1961.00\n1980.0\n2002.00\n2010\n▁▁▅▅▇\n\n\nGarageCars\n0\n1.00\n1.77\n0.75\n0\n1.00\n2.0\n2.00\n4\n▁▃▇▂▁\n\n\nGarageArea\n0\n1.00\n472.98\n213.80\n0\n334.50\n480.0\n576.00\n1418\n▂▇▃▁▁\n\n\nWoodDeckSF\n0\n1.00\n94.24\n125.34\n0\n0.00\n0.0\n168.00\n857\n▇▂▁▁▁\n\n\nOpenPorchSF\n0\n1.00\n46.66\n66.26\n0\n0.00\n25.0\n68.00\n547\n▇▁▁▁▁\n\n\nEnclosedPorch\n0\n1.00\n21.95\n61.12\n0\n0.00\n0.0\n0.00\n552\n▇▁▁▁▁\n\n\n3SsnPorch\n0\n1.00\n3.41\n29.32\n0\n0.00\n0.0\n0.00\n508\n▇▁▁▁▁\n\n\nScreenPorch\n0\n1.00\n15.06\n55.76\n0\n0.00\n0.0\n0.00\n480\n▇▁▁▁▁\n\n\nPoolArea\n0\n1.00\n2.76\n40.18\n0\n0.00\n0.0\n0.00\n738\n▇▁▁▁▁\n\n\nMiscVal\n0\n1.00\n43.49\n496.12\n0\n0.00\n0.0\n0.00\n15500\n▇▁▁▁▁\n\n\nMoSold\n0\n1.00\n6.32\n2.70\n1\n5.00\n6.0\n8.00\n12\n▃▆▇▃▃\n\n\nYrSold\n0\n1.00\n2007.82\n1.33\n2006\n2007.00\n2008.0\n2009.00\n2010\n▇▇▇▇▅\n\n\nSalePrice\n0\n1.00\n180921.20\n79442.50\n34900\n129975.00\n163000.0\n214000.00\n755000\n▇▅▁▁▁"
  },
  {
    "objectID": "blog/posts/ames-housing/index.html#data-transformation",
    "href": "blog/posts/ames-housing/index.html#data-transformation",
    "title": "Ames Housing Price Prediction",
    "section": "Data Transformation",
    "text": "Data Transformation\nAfter we inspect and match with the data description, some columns with missing data actually not really have missing data. For example with the column “Fence”, the value “na” in this column does not mean that the value does not exist but it means that there is no “fence” in the house so we can replace the value “na” with “no”.\n\names_train &lt;- ames_train |&gt;\n  mutate(across(starts_with(\"Bsmt\"), ~ replace_na(., \"No\")))\n\names_train &lt;- ames_train |&gt;\n  rename(GrgYrBlt = GarageYrBlt) |&gt;\n  mutate(across(starts_with(\"Garage\"), ~ replace_na(., \"No\"))) |&gt;\n  rename(GarageYrBlt = GrgYrBlt)\n\names_train &lt;- ames_train |&gt;\n  mutate(across(c(Alley, FireplaceQu, PoolQC, Fence, MiscFeature, MasVnrType), ~ replace_na(., \"No\")))\n\names_train &lt;- ames_train |&gt;\n  mutate(MasVnrArea = replace_na(MasVnrArea, 0))\n\names_train &lt;- ames_train |&gt;\n  mutate(across(where(is.character), as.factor))"
  },
  {
    "objectID": "blog/posts/ames-housing/index.html#explore-the-data",
    "href": "blog/posts/ames-housing/index.html#explore-the-data",
    "title": "Ames Housing Price Prediction",
    "section": "Explore the data",
    "text": "Explore the data\n\n ggplot(data = ames_train) +\n  geom_histogram(aes(x = SalePrice), bins = 50) +\n  scale_x_log10() \n\n\n\n\n\n\n\nggplot(data = ames_train) +\n  geom_bar(aes(x = OverallCond))"
  },
  {
    "objectID": "blog/posts/ames-housing/index.html#build-a-model",
    "href": "blog/posts/ames-housing/index.html#build-a-model",
    "title": "Ames Housing Price Prediction",
    "section": "Build a model",
    "text": "Build a model\n\nset.seed(123)\nhouse_split &lt;- initial_split(ames_train)\nhouse_train &lt;- training(house_split)\nhouse_test &lt;- testing(house_split)\n\nset.seed(88)\nhouse_fold &lt;- vfold_cv(ames_train, v = 10, repeats = 1)\n\nThis time, because there are so many features in the dataset, we will try dimensionality reduction using principal component analysis (PCA) and independent component analysis (ICA). We will make three recipes which are basic recipe, pca recipe, and ica recipe.\n\nbasic_recipe &lt;- recipe(SalePrice ~ . , data = house_train) |&gt; \n  update_role(Id, new_role = \"id\") |&gt; \n  step_impute_knn(all_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_corr(all_numeric_predictors(), threshold = 0.95) |&gt; \n  step_dummy(all_nominal_predictors()) \n\npca_recipe &lt;- recipe(SalePrice ~ . , data = house_train) |&gt;\n  update_role(Id, new_role = \"id\") |&gt; \n  step_impute_knn(all_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_corr(all_numeric_predictors(), threshold = 0.95) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_pca(all_predictors(), num_comp = 8) \n\nprep_pca &lt;- prep(pca_recipe) \n \nbaked_pca &lt;- bake(prep_pca, new_data = NULL)\n\nhead(baked_pca)\n\n# A tibble: 6 × 10\n     Id SalePrice   PC1   PC2    PC3    PC4     PC5    PC6     PC7    PC8\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1   415    228000  5.10 -2.91 -2.06   0.739 -1.12   -2.31   0.0254  2.18 \n2   463     62383  5.01  3.62  0.619 -1.32   0.0369 -0.529 -0.365   0.195\n3   179    501837  4.53 -6.36  3.42  -1.87   0.109   1.67  -0.0897  0.258\n4   526    176000  5.29 -1.56  0.638  2.27   2.39   -0.718 -0.982   0.366\n5   195    127000  5.21  3.13  1.16   0.152  0.613  -0.452  0.700  -0.614\n6   938    253000  5.44 -2.83 -1.13   1.93  -0.318  -0.463  0.339  -1.20 \n\nica_recipe &lt;- recipe(SalePrice ~ . , data = house_train) |&gt;\n  update_role(Id, new_role = \"id\") |&gt; \n  step_impute_knn(all_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_corr(all_numeric_predictors(), threshold = 0.95) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_ica(all_predictors() , num_comp = 8)\n\nprep_ica &lt;- prep(ica_recipe) \n\nbaked_ica &lt;- bake(prep_ica, new_data = NULL)\n\nhead(baked_ica)\n\n# A tibble: 6 × 10\n     Id SalePrice     IC1    IC2    IC3    IC4    IC5    IC6    IC7    IC8\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   415    228000  0.0347  2.63   0.730  0.667  0.428  1.54   1.10  -0.971\n2   463     62383 -0.193   0.606  0.316  0.352 -0.348 -0.485 -0.133  1.24 \n3   179    501837 -0.536  -1.50  -2.74  -0.147  0.961 -1.80   1.07  -1.15 \n4   526    176000  0.964   0.181  1.41   0.323 -0.497 -0.851  0.586 -1.34 \n5   195    127000  0.367  -0.516  0.556  0.508  0.439 -0.497 -0.623  0.905\n6   938    253000  0.684  -0.138  0.266  0.367 -0.134  1.11  -0.884 -1.30 \n\n\n\nbaked_pca |&gt; \n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_point()\n\n\n\n\n\n\n\nbaked_ica |&gt; \n  ggplot(aes(x = IC1, y = IC2)) +\n  geom_point()\n\n\n\n\n\n\n\n\nFor the model, we will build a single layer neural network and we also gonna tune the paramater for hidden units, penalty, epochs, and learn rate.\n\ndoParallel::registerDoParallel()\nmlp_model &lt;- mlp(hidden_units = tune(), penalty = tune(), epochs = tune(), learn_rate = tune()) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"brulee\")\n\nhouse_wf &lt;- workflow_set(preproc = list(pca = pca_recipe, ica = ica_recipe, basic = basic_recipe), models = list(mlp_model))\n\nhouse_res &lt;- workflow_map(house_wf, \n                          resamples = house_fold,\n                          seed = 999,\n                          grid = 5, \n                          control = control_grid(parallel_over = \"everything\", save_pred = TRUE))"
  },
  {
    "objectID": "blog/posts/ames-housing/index.html#evaluate-the-model",
    "href": "blog/posts/ames-housing/index.html#evaluate-the-model",
    "title": "Ames Housing Price Prediction",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\ncollect_metrics(house_res) \n\n# A tibble: 30 × 9\n   wflow_id .config       preproc model .metric .estimator    mean     n std_err\n   &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 pca_mlp  Preprocessor… recipe  mlp   rmse    standard   4.88e+4    10 5.56e+3\n 2 pca_mlp  Preprocessor… recipe  mlp   rsq     standard   6.28e-1    10 8.24e-2\n 3 pca_mlp  Preprocessor… recipe  mlp   rmse    standard   3.87e+4    10 5.18e+3\n 4 pca_mlp  Preprocessor… recipe  mlp   rsq     standard   7.71e-1    10 5.11e-2\n 5 pca_mlp  Preprocessor… recipe  mlp   rmse    standard   3.38e+4    10 3.06e+3\n 6 pca_mlp  Preprocessor… recipe  mlp   rsq     standard   8.22e-1    10 2.83e-2\n 7 pca_mlp  Preprocessor… recipe  mlp   rmse    standard   3.69e+4    10 3.32e+3\n 8 pca_mlp  Preprocessor… recipe  mlp   rsq     standard   7.90e-1    10 3.57e-2\n 9 pca_mlp  Preprocessor… recipe  mlp   rmse    standard   3.66e+4    10 4.81e+3\n10 pca_mlp  Preprocessor… recipe  mlp   rsq     standard   7.98e-1    10 4.56e-2\n# ℹ 20 more rows\n\nhouse_res |&gt; \n  rank_results() |&gt; \n  filter(.metric == 'rmse')\n\n# A tibble: 15 × 9\n   wflow_id  .config       .metric   mean std_err     n preprocessor model  rank\n   &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 basic_mlp Preprocessor… rmse    3.29e4   4392.    10 recipe       mlp       1\n 2 basic_mlp Preprocessor… rmse    3.36e4   4019.    10 recipe       mlp       2\n 3 pca_mlp   Preprocessor… rmse    3.38e4   3056.    10 recipe       mlp       3\n 4 basic_mlp Preprocessor… rmse    3.47e4   4458.    10 recipe       mlp       4\n 5 ica_mlp   Preprocessor… rmse    3.64e4   3293.    10 recipe       mlp       5\n 6 pca_mlp   Preprocessor… rmse    3.66e4   4811.    10 recipe       mlp       6\n 7 pca_mlp   Preprocessor… rmse    3.69e4   3323.    10 recipe       mlp       7\n 8 ica_mlp   Preprocessor… rmse    3.71e4   4168.    10 recipe       mlp       8\n 9 ica_mlp   Preprocessor… rmse    3.79e4   3987.    10 recipe       mlp       9\n10 ica_mlp   Preprocessor… rmse    3.82e4   4674.    10 recipe       mlp      10\n11 pca_mlp   Preprocessor… rmse    3.87e4   5180.    10 recipe       mlp      11\n12 basic_mlp Preprocessor… rmse    4.10e4   5132.    10 recipe       mlp      12\n13 pca_mlp   Preprocessor… rmse    4.88e4   5559.    10 recipe       mlp      13\n14 basic_mlp Preprocessor… rmse    5.31e4   9738.    10 recipe       mlp      14\n15 ica_mlp   Preprocessor… rmse    4.50e5 404814.    10 recipe       mlp      15\n\nautoplot(house_res, rank_metric = 'rmse',  metric = 'rmse' , select_best = TRUE, type = 'wflow_id' )\n\n\n\n\n\n\n\n\nThe results actually show that the basic recipe without using the dimension reduction have a better performance, but for this time we will use the pca because the results are close to basic recipe.\n\nbest_tune &lt;- house_res |&gt; \n  extract_workflow_set_result(id = 'pca_mlp') |&gt; \n  select_best(metric = 'rmse')\n\nbest_tune\n\n# A tibble: 1 × 5\n  hidden_units  penalty epochs learn_rate .config             \n         &lt;int&gt;    &lt;dbl&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n1            4 3.90e-10    247      0.289 Preprocessor1_Model3\n\nfinal_model &lt;- house_res |&gt; \n  extract_workflow('pca_mlp') |&gt; \n  finalize_workflow(best_tune) |&gt; \n  last_fit(split = house_split)\n\ncollect_metrics(final_model)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   32408.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1\n\n\n\nresults &lt;-  final_model |&gt; \n  collect_predictions() |&gt; \n  select(.pred, SalePrice)\n\nresults\n\n# A tibble: 365 × 2\n     .pred SalePrice\n     &lt;dbl&gt;     &lt;dbl&gt;\n 1 229737.    223500\n 2 255013.    279500\n 3 149332.    157000\n 4 319111.    325300\n 5 116824.    139400\n 6 255112.    230000\n 7 116824.    134800\n 8 116824.     68500\n 9 185617.    179900\n10 134922.    144000\n# ℹ 355 more rows\n\nresults |&gt; \n  ggplot(aes(x = SalePrice, y = .pred)) +\n  geom_point() +\n  geom_abline(lty = 2) +\n  coord_obs_pred()"
  },
  {
    "objectID": "blog/posts/ames-housing/index.html#predict-test-set-and-import-the-results",
    "href": "blog/posts/ames-housing/index.html#predict-test-set-and-import-the-results",
    "title": "Ames Housing Price Prediction",
    "section": "Predict test set and import the results",
    "text": "Predict test set and import the results\n\npred &lt;- extract_workflow(final_model) |&gt; \n  predict(ames_test)\n\npred\n\n# A tibble: 1,459 × 1\n     .pred\n     &lt;dbl&gt;\n 1 116824.\n 2 142262.\n 3 185944.\n 4 205764.\n 5 164173.\n 6 172864.\n 7 184034.\n 8 162081.\n 9 187495.\n10 116824.\n# ℹ 1,449 more rows\n\nsubmission &lt;- ames_test |&gt; \n  select(Id) |&gt; \n  bind_cols(pred) |&gt; \n  rename(SalePrice = .pred)\n\nsubmission\n\n# A tibble: 1,459 × 2\n      Id SalePrice\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  1461   116824.\n 2  1462   142262.\n 3  1463   185944.\n 4  1464   205764.\n 5  1465   164173.\n 6  1466   172864.\n 7  1467   184034.\n 8  1468   162081.\n 9  1469   187495.\n10  1470   116824.\n# ℹ 1,449 more rows\n\n\n\nwrite_csv(submission, \"submission.csv\")"
  },
  {
    "objectID": "blog/posts/spaceship-titanic/index.html",
    "href": "blog/posts/spaceship-titanic/index.html",
    "title": "Spaceship Titanic",
    "section": "",
    "text": "Impute the missing data for Spaceship Titanic and predict the passengers that transported to alternate dimension using Lightgbm\n\nIntroduction\nSpaceship Titanic is part of kaggle getting started competitions aims to introduce participants to basic machine learning concepts and facilitate networking within the Kaggle community. In this competition, our job is to predict which passengers are transported to an alternate dimension during the Spaceship Titanic’s collision with the spacetime anomaly. To assist in this prediction, we are provided with personal records retrieved from the ship’s damaged computer system.\nWe will build a classification model to predict which passengers are transported using Lightgbm. Lets get started!\n\n\nImport Library\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(bonsai)\nlibrary(vip)\nlibrary(lightgbm)\n\n\n\n\nLoad dataset\n\n\nCode\nspaceship_train &lt;- read_csv('train.csv')\nspaceship_test &lt;- read_csv('test.csv')\n\n\n\n\nCheck the data\n\n\nCode\nspaceship_train |&gt; head(10)\n\n\n# A tibble: 10 × 14\n   PassengerId HomePlanet CryoSleep Cabin Destination     Age VIP   RoomService\n   &lt;chr&gt;       &lt;chr&gt;      &lt;lgl&gt;     &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;lgl&gt;       &lt;dbl&gt;\n 1 0001_01     Europa     FALSE     B/0/P TRAPPIST-1e      39 FALSE           0\n 2 0002_01     Earth      FALSE     F/0/S TRAPPIST-1e      24 FALSE         109\n 3 0003_01     Europa     FALSE     A/0/S TRAPPIST-1e      58 TRUE           43\n 4 0003_02     Europa     FALSE     A/0/S TRAPPIST-1e      33 FALSE           0\n 5 0004_01     Earth      FALSE     F/1/S TRAPPIST-1e      16 FALSE         303\n 6 0005_01     Earth      FALSE     F/0/P PSO J318.5-22    44 FALSE           0\n 7 0006_01     Earth      FALSE     F/2/S TRAPPIST-1e      26 FALSE          42\n 8 0006_02     Earth      TRUE      G/0/S TRAPPIST-1e      28 FALSE           0\n 9 0007_01     Earth      FALSE     F/3/S TRAPPIST-1e      35 FALSE           0\n10 0008_01     Europa     TRUE      B/1/P 55 Cancri e      14 FALSE           0\n# ℹ 6 more variables: FoodCourt &lt;dbl&gt;, ShoppingMall &lt;dbl&gt;, Spa &lt;dbl&gt;,\n#   VRDeck &lt;dbl&gt;, Name &lt;chr&gt;, Transported &lt;lgl&gt;\n\n\nCode\nglimpse(spaceship_train)\n\n\nRows: 8,693\nColumns: 14\n$ PassengerId  &lt;chr&gt; \"0001_01\", \"0002_01\", \"0003_01\", \"0003_02\", \"0004_01\", \"0…\n$ HomePlanet   &lt;chr&gt; \"Europa\", \"Earth\", \"Europa\", \"Europa\", \"Earth\", \"Earth\", …\n$ CryoSleep    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FA…\n$ Cabin        &lt;chr&gt; \"B/0/P\", \"F/0/S\", \"A/0/S\", \"A/0/S\", \"F/1/S\", \"F/0/P\", \"F/…\n$ Destination  &lt;chr&gt; \"TRAPPIST-1e\", \"TRAPPIST-1e\", \"TRAPPIST-1e\", \"TRAPPIST-1e…\n$ Age          &lt;dbl&gt; 39, 24, 58, 33, 16, 44, 26, 28, 35, 14, 34, 45, 32, 48, 2…\n$ VIP          &lt;lgl&gt; FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ RoomService  &lt;dbl&gt; 0, 109, 43, 0, 303, 0, 42, 0, 0, 0, 0, 39, 73, 719, 8, 32…\n$ FoodCourt    &lt;dbl&gt; 0, 9, 3576, 1283, 70, 483, 1539, 0, 785, 0, 0, 7295, 0, 1…\n$ ShoppingMall &lt;dbl&gt; 0, 25, 0, 371, 151, 0, 3, 0, 17, 0, NA, 589, 1123, 65, 12…\n$ Spa          &lt;dbl&gt; 0, 549, 6715, 3329, 565, 291, 0, 0, 216, 0, 0, 110, 0, 0,…\n$ VRDeck       &lt;dbl&gt; 0, 44, 49, 193, 2, 0, 0, NA, 0, 0, 0, 124, 113, 24, 7, 0,…\n$ Name         &lt;chr&gt; \"Maham Ofracculy\", \"Juanna Vines\", \"Altark Susent\", \"Sola…\n$ Transported  &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n\n\n\n\nCheck missing values\n\n\nCode\nspaceship_train |&gt; summarise_all(~sum(is.na(.))) \n\n\n# A tibble: 1 × 14\n  PassengerId HomePlanet CryoSleep Cabin Destination   Age   VIP RoomService\n        &lt;int&gt;      &lt;int&gt;     &lt;int&gt; &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;       &lt;int&gt;\n1           0        201       217   199         182   179   203         181\n# ℹ 6 more variables: FoodCourt &lt;int&gt;, ShoppingMall &lt;int&gt;, Spa &lt;int&gt;,\n#   VRDeck &lt;int&gt;, Name &lt;int&gt;, Transported &lt;int&gt;\n\n\nCode\nspaceship_test |&gt; summarise_all(~sum(is.na(.)))\n\n\n# A tibble: 1 × 13\n  PassengerId HomePlanet CryoSleep Cabin Destination   Age   VIP RoomService\n        &lt;int&gt;      &lt;int&gt;     &lt;int&gt; &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;       &lt;int&gt;\n1           0         87        93   100          92    91    93          82\n# ℹ 5 more variables: FoodCourt &lt;int&gt;, ShoppingMall &lt;int&gt;, Spa &lt;int&gt;,\n#   VRDeck &lt;int&gt;, Name &lt;int&gt;\n\n\n\n\nData transformation\n\n\nCode\nspaceship_train &lt;-  spaceship_train |&gt; \n  separate_wider_delim(cols = Cabin, delim = '/', names = c('deck', 'number', 'side')) \n\nspaceship_test &lt;-  spaceship_test |&gt; \n  separate_wider_delim(cols = Cabin, delim = '/', names = c('deck', 'number', 'side'))  \n\nspaceship_train &lt;-  spaceship_train |&gt; \n  mutate(across(c(HomePlanet, CryoSleep, deck, side, Destination, VIP, Transported), as.factor)) |&gt; \n  mutate(number = as.numeric(number)) |&gt; \n  select(-Name, -PassengerId)\n\nspaceship_test &lt;-  spaceship_test |&gt; \n  mutate(across(c(HomePlanet, CryoSleep, deck, side, Destination, VIP), as.factor)) |&gt; \n  mutate(number = as.numeric(number)) |&gt; \n  select(-Name)\n\nstr(spaceship_train)\n\n\ntibble [8,693 × 14] (S3: tbl_df/tbl/data.frame)\n $ HomePlanet  : Factor w/ 3 levels \"Earth\",\"Europa\",..: 2 1 2 2 1 1 1 1 1 2 ...\n $ CryoSleep   : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 1 1 1 1 1 2 1 2 ...\n $ deck        : Factor w/ 8 levels \"A\",\"B\",\"C\",\"D\",..: 2 6 1 1 6 6 6 7 6 2 ...\n $ number      : num [1:8693] 0 0 0 0 1 0 2 0 3 1 ...\n $ side        : Factor w/ 2 levels \"P\",\"S\": 1 2 2 2 2 1 2 2 2 1 ...\n $ Destination : Factor w/ 3 levels \"55 Cancri e\",..: 3 3 3 3 3 2 3 3 3 1 ...\n $ Age         : num [1:8693] 39 24 58 33 16 44 26 28 35 14 ...\n $ VIP         : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 2 1 1 1 1 1 1 1 ...\n $ RoomService : num [1:8693] 0 109 43 0 303 0 42 0 0 0 ...\n $ FoodCourt   : num [1:8693] 0 9 3576 1283 70 ...\n $ ShoppingMall: num [1:8693] 0 25 0 371 151 0 3 0 17 0 ...\n $ Spa         : num [1:8693] 0 549 6715 3329 565 ...\n $ VRDeck      : num [1:8693] 0 44 49 193 2 0 0 NA 0 0 ...\n $ Transported : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 2 1 1 2 2 2 2 2 2 ...\n\n\n\n\nExplore the data\n\n\nCode\nspaceship_train |&gt; \n  drop_na(CryoSleep) |&gt; \n  ggplot(aes(x = CryoSleep, fill = Transported)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Paired\") \n\n\n\n\n\n\n\n\n\nCode\nspaceship_train |&gt; \n  ggplot(aes(x = VIP, fill = Transported)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Paired\") \n\n\n\n\n\n\n\n\n\nCode\nspaceship_train |&gt; \n  ggplot(aes(x = HomePlanet, fill = Transported)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Paired\") \n\n\n\n\n\n\n\n\n\nCode\nspaceship_train |&gt; \n  ggplot(aes(x = Transported,  Age)) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nspaceship_train |&gt; \n  ggplot(aes(x = RoomService, y = FoodCourt, color = Transported)) +\n  geom_point(alpha = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nspaceship_train |&gt; \n  ggplot(aes(x = VRDeck, y = Spa, color = Transported)) +\n  geom_point(alpha = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBuild a model\n\n\nCode\nset.seed(789)\nspaceship_split &lt;- initial_split(spaceship_train, prop = 0.8, strata = Transported)\ntrain &lt;- training(spaceship_split)\ntest &lt;- testing(spaceship_split)\n\nset.seed(777)\nspaceship_fold &lt;- vfold_cv(train, v = 10, strata = Transported)\n\nspaceship_recipe &lt;- recipe(Transported ~ ., data = train) |&gt; \n  step_impute_knn(CryoSleep) |&gt; \n  step_impute_median(all_numeric_predictors(), -VRDeck, -Spa) |&gt; \n  step_impute_linear(VRDeck, Spa, impute_with = imp_vars(RoomService, FoodCourt, ShoppingMall)) |&gt; \n  step_zv(all_predictors()) \n  \nbt_spec &lt;- boost_tree(trees = 200, mtry = tune(), min_n = tune(), tree_depth = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"lightgbm\")\n\nspaceship_wf &lt;- workflow() |&gt; \n  add_recipe(spaceship_recipe) |&gt; \n  add_model(bt_spec)\n\nset.seed(1234)\ndoParallel::registerDoParallel()\nbt_fit &lt;- spaceship_wf |&gt; \n  tune_grid(resamples = spaceship_fold,\n            grid = 5,\n            control = control_resamples(save_pred = TRUE))\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n\n\nEvaluate the model\n\n\nCode\ncollect_metrics(bt_fit)\n\n\n# A tibble: 10 × 9\n    mtry min_n tree_depth .metric  .estimator  mean     n std_err .config       \n   &lt;int&gt; &lt;int&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         \n 1     6     7         12 accuracy binary     0.807    10 0.00578 Preprocessor1…\n 2     6     7         12 roc_auc  binary     0.899    10 0.00445 Preprocessor1…\n 3    12    16          9 accuracy binary     0.805    10 0.00686 Preprocessor1…\n 4    12    16          9 roc_auc  binary     0.896    10 0.00493 Preprocessor1…\n 5     3    31          3 accuracy binary     0.803    10 0.00455 Preprocessor1…\n 6     3    31          3 roc_auc  binary     0.896    10 0.00390 Preprocessor1…\n 7    10    37          4 accuracy binary     0.811    10 0.00447 Preprocessor1…\n 8    10    37          4 roc_auc  binary     0.901    10 0.00439 Preprocessor1…\n 9     5    21         12 accuracy binary     0.805    10 0.00700 Preprocessor1…\n10     5    21         12 roc_auc  binary     0.898    10 0.00445 Preprocessor1…\n\n\nCode\nshow_best(bt_fit, metric = \"roc_auc\")\n\n\n# A tibble: 5 × 9\n   mtry min_n tree_depth .metric .estimator  mean     n std_err .config         \n  &lt;int&gt; &lt;int&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1    10    37          4 roc_auc binary     0.901    10 0.00439 Preprocessor1_M…\n2     6     7         12 roc_auc binary     0.899    10 0.00445 Preprocessor1_M…\n3     5    21         12 roc_auc binary     0.898    10 0.00445 Preprocessor1_M…\n4    12    16          9 roc_auc binary     0.896    10 0.00493 Preprocessor1_M…\n5     3    31          3 roc_auc binary     0.896    10 0.00390 Preprocessor1_M…\n\n\nCode\nbest_tune &lt;- select_best(bt_fit, metric = 'roc_auc')\n\nfinal_wf &lt;- finalize_workflow(spaceship_wf, best_tune)\n\nspaceship_final &lt;- final_wf |&gt; \n  last_fit(spaceship_split)\n\ncollect_metrics(spaceship_final)\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.816 Preprocessor1_Model1\n2 roc_auc  binary         0.905 Preprocessor1_Model1\n\n\nCode\nresults &lt;- collect_predictions(spaceship_final)\n\nresults |&gt; \n  conf_mat(truth = Transported, estimate = .pred_class) \n\n\n          Truth\nPrediction FALSE TRUE\n     FALSE   707  164\n     TRUE    156  712\n\n\nCode\nprecision(results, truth = Transported, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision binary         0.812\n\n\nCode\nf_meas(results, truth = Transported, estimate = .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  binary         0.815\n\n\nCode\nspaceship_final |&gt; \n  extract_fit_parsnip() |&gt; \n  vip(num_features = 8, aesthetics = list(fill = \"midnightblue\"))\n\n\n\n\n\n\n\n\n\nCode\nfinal_wf &lt;- extract_workflow(spaceship_final)\n\npred &lt;- predict(final_wf, spaceship_test)\n\npred &lt;-  pred |&gt; \n  mutate(Transported = case_match(.pred_class, \"TRUE\" ~ \"True\",\n            \"FALSE\" ~ \"False\")) |&gt; \n  select(-.pred_class)\npred\n\n\n# A tibble: 4,277 × 1\n   Transported\n   &lt;chr&gt;      \n 1 True       \n 2 False      \n 3 True       \n 4 True       \n 5 True       \n 6 True       \n 7 True       \n 8 True       \n 9 True       \n10 True       \n# ℹ 4,267 more rows\n\n\n\n\nExport dataset for submission\n\n\nCode\nsubmission &lt;- spaceship_test |&gt; \n  select(PassengerId) |&gt; \n  bind_cols(pred) \n\nsubmission\n\n\n# A tibble: 4,277 × 2\n   PassengerId Transported\n   &lt;chr&gt;       &lt;chr&gt;      \n 1 0013_01     True       \n 2 0018_01     False      \n 3 0019_01     True       \n 4 0021_01     True       \n 5 0023_01     True       \n 6 0027_01     True       \n 7 0029_01     True       \n 8 0032_01     True       \n 9 0032_02     True       \n10 0033_01     True       \n# ℹ 4,267 more rows\n\n\nCode\nwrite_csv(submission, file = 'submission-2.csv')"
  },
  {
    "objectID": "blog/posts/book-dashboard/index.html",
    "href": "blog/posts/book-dashboard/index.html",
    "title": "Books Dashboard",
    "section": "",
    "text": "Building a dashboard using quarto and gt package for the table"
  },
  {
    "objectID": "blog/posts/book-dashboard/index.html#mybook-dashboard",
    "href": "blog/posts/book-dashboard/index.html#mybook-dashboard",
    "title": "Books Dashboard",
    "section": "MyBook Dashboard",
    "text": "MyBook Dashboard\nHello, for this time i built dashboard to track the books that i have read. To see the dashboard you can visit https://nusaseldi.github.io/mybook_dashboard.\nIn the dashboard, you can see how many books that i already read and the details about the book from title, author, genre, and pages. I also included the the rating that i got from goodreads. I hope I can read books consistently every month!"
  }
]