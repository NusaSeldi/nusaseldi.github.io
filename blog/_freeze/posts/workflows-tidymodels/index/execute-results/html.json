{
  "hash": "828fd9470aa1876ae7b861eafc31e071",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Build several models with workflowsets in tidymodels\"\nauthor: \"Nusa Seldi\"\ndate: \"2024-05-13\"\ncategories: [code, analysis, workflows, tidymodels, mtcars]\nimage: \"image.jpg\"\n---\n\n\n## Introduction\n\nTrying workflowsets package with mtcars dataset to create collection of models.\n\nMotor Trend Car Road Tests (mtcars) contains the data from *Motor Trend* US magazine about fuel consumption and other aspects of automobile design and performance for 32 automobiles (1973-74 models).\n\nThis time, our goal is to predict the Miles per gallon (mpg) of 32 automobiles. We will utilize tidymodels' workflow_set function to build several regression models at once: linear model, support vector machine, and xgboost.\n\n## Import Library\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(skimr)\n```\n:::\n\n\n## Load the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar_df <- mtcars\n\nglimpse(car_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n```\n\n\n:::\n\n```{.r .cell-code}\nskim(car_df)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |       |\n|:------------------------|:------|\n|Name                     |car_df |\n|Number of rows           |32     |\n|Number of columns        |11     |\n|_______________________  |       |\n|Column type frequency:   |       |\n|numeric                  |11     |\n|________________________ |       |\n|Group variables          |None   |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|    p0|    p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|------:|-----:|------:|------:|------:|------:|:-----|\n|mpg           |         0|             1|  20.09|   6.03| 10.40|  15.43|  19.20|  22.80|  33.90|▃▇▅▁▂ |\n|cyl           |         0|             1|   6.19|   1.79|  4.00|   4.00|   6.00|   8.00|   8.00|▆▁▃▁▇ |\n|disp          |         0|             1| 230.72| 123.94| 71.10| 120.83| 196.30| 326.00| 472.00|▇▃▃▃▂ |\n|hp            |         0|             1| 146.69|  68.56| 52.00|  96.50| 123.00| 180.00| 335.00|▇▇▆▃▁ |\n|drat          |         0|             1|   3.60|   0.53|  2.76|   3.08|   3.70|   3.92|   4.93|▇▃▇▅▁ |\n|wt            |         0|             1|   3.22|   0.98|  1.51|   2.58|   3.33|   3.61|   5.42|▃▃▇▁▂ |\n|qsec          |         0|             1|  17.85|   1.79| 14.50|  16.89|  17.71|  18.90|  22.90|▃▇▇▂▁ |\n|vs            |         0|             1|   0.44|   0.50|  0.00|   0.00|   0.00|   1.00|   1.00|▇▁▁▁▆ |\n|am            |         0|             1|   0.41|   0.50|  0.00|   0.00|   0.00|   1.00|   1.00|▇▁▁▁▆ |\n|gear          |         0|             1|   3.69|   0.74|  3.00|   3.00|   4.00|   4.00|   5.00|▇▁▆▁▂ |\n|carb          |         0|             1|   2.81|   1.62|  1.00|   2.00|   2.00|   4.00|   8.00|▇▂▅▁▁ |\n\n\n:::\n:::\n\n\nIt can be seen that the dataset consists of 32 automobiles with 11 variables. All data types are numeric and there are no missing values in the dataset.\n\n## Preprocessing\n\nFor preprocessing, we will change two variables, am (transmission) and vs (engine). We will transform the format to factor and also change the labels of the values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar_df <- car_df |> \n  mutate( am = case_match(am, 1 ~ \"manual\", .default = 'automatic'),\n          vs = case_match(vs, 1 ~ 'straight', .default = 'v-shaped'),\n          am = as.factor(am),\n          vs = as.factor(vs))\n```\n:::\n\n\n## Explore the data\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-5.png){width=672}\n:::\n:::\n\n\n## Build a Model\n\nBefore we build a model, we will divide the data into training set and test set with a ratio of 80:20. For feature engineering, we will normalize the data for numeric data and create dummy variables for nominal data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\ncar_split <- initial_split(car_df, prop = 0.8)\ncar_train <- training(car_split)\ncar_test <- testing(car_split)\n\nset.seed(80)\ncar_fold <- bootstraps(car_train, times = 10)\n\ncar_recipe <- recipe(mpg ~ ., data = car_train) |> \n  step_normalize(all_numeric_predictors()) |> \n  step_dummy(all_nominal_predictors())\n\nlm_spec <- linear_reg() |> \n  set_mode('regression') |> \n  set_engine('stan')\n\nxgb_spec <- boost_tree() |> \n  set_mode('regression') |> \n  set_engine('xgboost')\n\nsvm_spec <- svm_linear() |> \n  set_mode('regression') |> \n  set_engine('kernlab')\n\nwf_set <- workflow_set(preproc = list(basic = car_recipe),\n                       models = list(lm = lm_spec,\n                                     xgboost = xgb_spec,\n                                     svm = svm_spec))\n\nwf_set_fit <- workflow_map(wf_set,\n                           resamples = car_fold,\n                           seed = 123,\n                           control = control_grid(save_pred = TRUE, save_workflow = TRUE ,parallel_over = \"everything\"))\n```\n:::\n\n\n## Evaluate the model\n\nFor the evaluation, we will use metric *rmse* to estimate our model performance. From the three models, we choose the best model according to metric rmse and fit the final model to the training set and evaluate the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_set_fit |> collect_metrics(summarize = T) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n  wflow_id      .config     preproc model .metric .estimator  mean     n std_err\n  <chr>         <chr>       <chr>   <chr> <chr>   <chr>      <dbl> <int>   <dbl>\n1 basic_lm      Preprocess… recipe  line… rmse    standard   5.57     10  0.245 \n2 basic_lm      Preprocess… recipe  line… rsq     standard   0.433    10  0.0540\n3 basic_xgboost Preprocess… recipe  boos… rmse    standard   3.57     10  0.193 \n4 basic_xgboost Preprocess… recipe  boos… rsq     standard   0.728    10  0.0265\n5 basic_svm     Preprocess… recipe  svm_… rmse    standard   4.46     10  0.272 \n6 basic_svm     Preprocess… recipe  svm_… rsq     standard   0.584    10  0.0486\n```\n\n\n:::\n\n```{.r .cell-code}\nwf_set_fit |> \n  rank_results() |> \n  filter(.metric == 'rmse')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 9\n  wflow_id      .config     .metric  mean std_err     n preprocessor model  rank\n  <chr>         <chr>       <chr>   <dbl>   <dbl> <int> <chr>        <chr> <int>\n1 basic_xgboost Preprocess… rmse     3.57   0.193    10 recipe       boos…     1\n2 basic_svm     Preprocess… rmse     4.46   0.272    10 recipe       svm_…     2\n3 basic_lm      Preprocess… rmse     5.57   0.245    10 recipe       line…     3\n```\n\n\n:::\n\n```{.r .cell-code}\nautoplot(wf_set_fit, rank_metric = 'rmse', metric = 'rmse', select_best = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbest_result <- wf_set_fit |> \n  extract_workflow_set_result(id = 'basic_xgboost') |> \n  select_best(metric = 'rmse')\n\nxgboost_result <- wf_set_fit |> \n  extract_workflow('basic_xgboost') |> \n  finalize_workflow(best_result) |> \n  last_fit(split = car_split)\n\ncollect_metrics(xgboost_result) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       1.69  Preprocessor1_Model1\n2 rsq     standard       0.888 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code}\npredicted <- xgboost_result |> \n  collect_predictions()\n\npredicted |> \n  select(.pred, mpg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 2\n  .pred   mpg\n  <dbl> <dbl>\n1  21.7  22.8\n2  17.6  19.2\n3  17.1  15.2\n4  10.4  10.4\n5  23.2  26  \n6  18.4  19.7\n7  23.0  21.4\n```\n\n\n:::\n\n```{.r .cell-code}\npredicted |> \n  ggplot(aes(x = mpg, y = .pred)) +\n  geom_point() +\n  geom_abline(lty = 2) +\n  coord_obs_pred()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}